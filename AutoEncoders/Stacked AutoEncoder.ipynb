{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"ml-1m/movies.dat\",sep = '::',header=None, engine='python',encoding='latin-1')\n",
    "users = pd.read_csv(\"ml-1m/users.dat\",sep = '::',header=None, engine='python',encoding='latin-1')\n",
    "ratings = pd.read_csv(\"ml-1m/ratings.dat\",sep = '::',header=None, engine='python',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        1         2         3 876893171]\n",
      " [        1         3         4 878542960]\n",
      " [        1         4         3 876893119]\n",
      " ...\n",
      " [      943      1188         3 888640250]\n",
      " [      943      1228         3 888640275]\n",
      " [      943      1330         3 888692465]]\n"
     ]
    }
   ],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base',delimiter='\\t')\n",
    "training_set = np.array(training_set, dtype='int')\n",
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        1        10         3 875693118]\n",
      " [        1        12         5 878542960]\n",
      " [        1        14         5 874965706]\n",
      " ...\n",
      " [      459       934         3 879563639]\n",
      " [      460        10         3 882912371]\n",
      " [      462       682         5 886365231]]\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter='\\t')\n",
    "test_set = np.array(test_set, dtype='int')\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data into an array with users in lines and movies in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20,10)\n",
    "        self.fc3 = nn.Linear(10,20)\n",
    "        self.fc4 = nn.Linear(20,nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.7658592112372924\n",
      "epoch: 2 loss: 1.0964134637180143\n",
      "epoch: 3 loss: 1.0536830584773265\n",
      "epoch: 4 loss: 1.0382909949295196\n",
      "epoch: 5 loss: 1.0310620044676098\n",
      "epoch: 6 loss: 1.0267053320579758\n",
      "epoch: 7 loss: 1.0239449587877694\n",
      "epoch: 8 loss: 1.0219318056099929\n",
      "epoch: 9 loss: 1.0210546313072988\n",
      "epoch: 10 loss: 1.0195844621216035\n",
      "epoch: 11 loss: 1.0189739426598676\n",
      "epoch: 12 loss: 1.0185247587317685\n",
      "epoch: 13 loss: 1.018018790616514\n",
      "epoch: 14 loss: 1.0176168652752278\n",
      "epoch: 15 loss: 1.0171217448893952\n",
      "epoch: 16 loss: 1.01705456736491\n",
      "epoch: 17 loss: 1.016723847514434\n",
      "epoch: 18 loss: 1.01673183166667\n",
      "epoch: 19 loss: 1.0163626250834072\n",
      "epoch: 20 loss: 1.0161124697718142\n",
      "epoch: 21 loss: 1.0159719941030019\n",
      "epoch: 22 loss: 1.0160097062376812\n",
      "epoch: 23 loss: 1.0157539387107013\n",
      "epoch: 24 loss: 1.0160778635089203\n",
      "epoch: 25 loss: 1.0154315955196054\n",
      "epoch: 26 loss: 1.0156884457278912\n",
      "epoch: 27 loss: 1.0152938983309154\n",
      "epoch: 28 loss: 1.0151347331629068\n",
      "epoch: 29 loss: 1.0128958788173341\n",
      "epoch: 30 loss: 1.0113460701834975\n",
      "epoch: 31 loss: 1.0102170309930132\n",
      "epoch: 32 loss: 1.0072447534900495\n",
      "epoch: 33 loss: 1.0076053703908274\n",
      "epoch: 34 loss: 1.0038670455334544\n",
      "epoch: 35 loss: 1.0042614782465782\n",
      "epoch: 36 loss: 1.000090228528057\n",
      "epoch: 37 loss: 0.9984416289215218\n",
      "epoch: 38 loss: 0.9964568967797438\n",
      "epoch: 39 loss: 0.9945466341824873\n",
      "epoch: 40 loss: 0.9932256741136811\n",
      "epoch: 41 loss: 0.994267804464547\n",
      "epoch: 42 loss: 0.990334870104054\n",
      "epoch: 43 loss: 0.9907612429684141\n",
      "epoch: 44 loss: 0.9872210948176107\n",
      "epoch: 45 loss: 0.9874448968518812\n",
      "epoch: 46 loss: 0.9843710423854416\n",
      "epoch: 47 loss: 0.9873396625197632\n",
      "epoch: 48 loss: 0.9848906531206317\n",
      "epoch: 49 loss: 0.9831883040494095\n",
      "epoch: 50 loss: 0.9767249198610994\n",
      "epoch: 51 loss: 0.979011892115417\n",
      "epoch: 52 loss: 0.9785823683732553\n",
      "epoch: 53 loss: 0.9750706914733935\n",
      "epoch: 54 loss: 0.9738268045863184\n",
      "epoch: 55 loss: 0.9788346366265035\n",
      "epoch: 56 loss: 0.9758159318475059\n",
      "epoch: 57 loss: 0.9725491405160233\n",
      "epoch: 58 loss: 0.9683177054053289\n",
      "epoch: 59 loss: 0.9699917439964473\n",
      "epoch: 60 loss: 0.9660894828814179\n",
      "epoch: 61 loss: 0.9629753967790345\n",
      "epoch: 62 loss: 0.9615070099601403\n",
      "epoch: 63 loss: 0.9665196415621718\n",
      "epoch: 64 loss: 0.9626544733843979\n",
      "epoch: 65 loss: 0.9635951604992226\n",
      "epoch: 66 loss: 0.9615797809440636\n",
      "epoch: 67 loss: 0.9662021467249986\n",
      "epoch: 68 loss: 0.963087438537248\n",
      "epoch: 69 loss: 0.9627258456210002\n",
      "epoch: 70 loss: 0.957177344724902\n",
      "epoch: 71 loss: 0.9583583274061176\n",
      "epoch: 72 loss: 0.9565082218414215\n",
      "epoch: 73 loss: 0.9559427521551159\n",
      "epoch: 74 loss: 0.9560573036922834\n",
      "epoch: 75 loss: 0.9533628289563852\n",
      "epoch: 76 loss: 0.9514201673308083\n",
      "epoch: 77 loss: 0.9533128388080159\n",
      "epoch: 78 loss: 0.9504930165988232\n",
      "epoch: 79 loss: 0.9505018242350245\n",
      "epoch: 80 loss: 0.9473019470540617\n",
      "epoch: 81 loss: 0.9490631228819287\n",
      "epoch: 82 loss: 0.9475841565186718\n",
      "epoch: 83 loss: 0.9492528130374263\n",
      "epoch: 84 loss: 0.944717221093655\n",
      "epoch: 85 loss: 0.9457758711640412\n",
      "epoch: 86 loss: 0.9425666681813999\n",
      "epoch: 87 loss: 0.9439571376640037\n",
      "epoch: 88 loss: 0.942216106073637\n",
      "epoch: 89 loss: 0.9434075952236296\n",
      "epoch: 90 loss: 0.9405623593312936\n",
      "epoch: 91 loss: 0.941483671053729\n",
      "epoch: 92 loss: 0.9395316314282711\n",
      "epoch: 93 loss: 0.9400860995008621\n",
      "epoch: 94 loss: 0.9376596032256519\n",
      "epoch: 95 loss: 0.9394916746900361\n",
      "epoch: 96 loss: 0.9368167637373832\n",
      "epoch: 97 loss: 0.9379312265810359\n",
      "epoch: 98 loss: 0.9364189945241077\n",
      "epoch: 99 loss: 0.9370227292631723\n",
      "epoch: 100 loss: 0.935130498069261\n",
      "epoch: 101 loss: 0.9364421700900767\n",
      "epoch: 102 loss: 0.9344944358235594\n",
      "epoch: 103 loss: 0.9357190462358053\n",
      "epoch: 104 loss: 0.9337582692913757\n",
      "epoch: 105 loss: 0.9349588478812554\n",
      "epoch: 106 loss: 0.9332692070072727\n",
      "epoch: 107 loss: 0.9348001969195984\n",
      "epoch: 108 loss: 0.9327810423703383\n",
      "epoch: 109 loss: 0.9334802520491301\n",
      "epoch: 110 loss: 0.9313881699798405\n",
      "epoch: 111 loss: 0.932201033930781\n",
      "epoch: 112 loss: 0.9303461876754894\n",
      "epoch: 113 loss: 0.931518388858754\n",
      "epoch: 114 loss: 0.9298919838406717\n",
      "epoch: 115 loss: 0.9307765637180893\n",
      "epoch: 116 loss: 0.9293742764980267\n",
      "epoch: 117 loss: 0.9298181883354131\n",
      "epoch: 118 loss: 0.9282389235253526\n",
      "epoch: 119 loss: 0.9295361616199737\n",
      "epoch: 120 loss: 0.9278215361423966\n",
      "epoch: 121 loss: 0.9281092362563321\n",
      "epoch: 122 loss: 0.9268223298587901\n",
      "epoch: 123 loss: 0.927938122434214\n",
      "epoch: 124 loss: 0.9263992615764849\n",
      "epoch: 125 loss: 0.9269646677082609\n",
      "epoch: 126 loss: 0.9258319354185901\n",
      "epoch: 127 loss: 0.926321561065352\n",
      "epoch: 128 loss: 0.9249266823074412\n",
      "epoch: 129 loss: 0.9258548559541726\n",
      "epoch: 130 loss: 0.9246632756808766\n",
      "epoch: 131 loss: 0.9252577568940588\n",
      "epoch: 132 loss: 0.9241071270252451\n",
      "epoch: 133 loss: 0.9247113539536016\n",
      "epoch: 134 loss: 0.9234332849833217\n",
      "epoch: 135 loss: 0.9239821783564292\n",
      "epoch: 136 loss: 0.9227757281121405\n",
      "epoch: 137 loss: 0.9232416991478738\n",
      "epoch: 138 loss: 0.9221944432212879\n",
      "epoch: 139 loss: 0.9226923620194467\n",
      "epoch: 140 loss: 0.9216533045385155\n",
      "epoch: 141 loss: 0.9220344529501917\n",
      "epoch: 142 loss: 0.9208539868121391\n",
      "epoch: 143 loss: 0.9217970137800704\n",
      "epoch: 144 loss: 0.9208147707782278\n",
      "epoch: 145 loss: 0.9219416437786833\n",
      "epoch: 146 loss: 0.920969676553174\n",
      "epoch: 147 loss: 0.9210924124056725\n",
      "epoch: 148 loss: 0.9199837558188323\n",
      "epoch: 149 loss: 0.9204581922330228\n",
      "epoch: 150 loss: 0.919812155586887\n",
      "epoch: 151 loss: 0.9199510528148163\n",
      "epoch: 152 loss: 0.9191018755188345\n",
      "epoch: 153 loss: 0.9193740332920463\n",
      "epoch: 154 loss: 0.91872927865587\n",
      "epoch: 155 loss: 0.919215639754933\n",
      "epoch: 156 loss: 0.9183498483707123\n",
      "epoch: 157 loss: 0.9192064374761985\n",
      "epoch: 158 loss: 0.9182397518353153\n",
      "epoch: 159 loss: 0.9181927066457463\n",
      "epoch: 160 loss: 0.9174217350911869\n",
      "epoch: 161 loss: 0.9182020725461615\n",
      "epoch: 162 loss: 0.9168389952234456\n",
      "epoch: 163 loss: 0.917263457715108\n",
      "epoch: 164 loss: 0.9164519357275662\n",
      "epoch: 165 loss: 0.9170054986477485\n",
      "epoch: 166 loss: 0.916292929277233\n",
      "epoch: 167 loss: 0.9167392059473161\n",
      "epoch: 168 loss: 0.9156166642144402\n",
      "epoch: 169 loss: 0.916144086775387\n",
      "epoch: 170 loss: 0.9155584152220135\n",
      "epoch: 171 loss: 0.9158003043583729\n",
      "epoch: 172 loss: 0.9152287457947\n",
      "epoch: 173 loss: 0.9155782827739049\n",
      "epoch: 174 loss: 0.9147517540308625\n",
      "epoch: 175 loss: 0.9151829872864788\n",
      "epoch: 176 loss: 0.9146688866858792\n",
      "epoch: 177 loss: 0.9147833581882102\n",
      "epoch: 178 loss: 0.9142118047094454\n",
      "epoch: 179 loss: 0.9143452670664249\n",
      "epoch: 180 loss: 0.9136975845932274\n",
      "epoch: 181 loss: 0.9139501255004076\n",
      "epoch: 182 loss: 0.9131487436387943\n",
      "epoch: 183 loss: 0.913676655958702\n",
      "epoch: 184 loss: 0.9131139056521306\n",
      "epoch: 185 loss: 0.9131041524226486\n",
      "epoch: 186 loss: 0.9128892020756115\n",
      "epoch: 187 loss: 0.9129411791138349\n",
      "epoch: 188 loss: 0.9120956185037897\n",
      "epoch: 189 loss: 0.9124167134933077\n",
      "epoch: 190 loss: 0.9118069468243106\n",
      "epoch: 191 loss: 0.9118461930900741\n",
      "epoch: 192 loss: 0.9113125888117152\n",
      "epoch: 193 loss: 0.9112523533832514\n",
      "epoch: 194 loss: 0.9108410525208602\n",
      "epoch: 195 loss: 0.9102441989692809\n",
      "epoch: 196 loss: 0.91026390027953\n",
      "epoch: 197 loss: 0.9099271126159202\n",
      "epoch: 198 loss: 0.9095435800165695\n",
      "epoch: 199 loss: 0.9101802067338713\n",
      "epoch: 200 loss: 0.9092869198765081\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0 :\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9548043652479968\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "        s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
